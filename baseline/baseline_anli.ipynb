{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline_anli.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oak8Ap04va1e"
      },
      "source": [
        "# Evaluation Method:\n",
        "As there are no actual labels but only *more probable* hypothesis out of two possible hypotheses, simple binary accuracy is well suited for this task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUYoX6QyvQ8v"
      },
      "source": [
        "class Accuracy:\n",
        "    \"\"\"\n",
        "    A class representing simple accuracy metric.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    # Calculate accuracy percentage between two lists\n",
        "    def binary_accuracy(self, gold_labels, predicted_labels):\n",
        "      \"\"\"\n",
        "      Args:\n",
        "        gold_labels: ground truth labels\n",
        "        predicted_labels: predictions from the model\n",
        "      \n",
        "      Returns:\n",
        "        int: accuracy\n",
        "      \"\"\"\n",
        "      correct = 0\n",
        "      for i in range(len(gold_labels)):\n",
        "        if gold_labels[i] == predicted_labels[i]:\n",
        "          correct += 1\n",
        "      return correct / float(len(gold_labels)) * 100.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nsjqu6-Kmpl"
      },
      "source": [
        "Test the accuracy implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoXFFanFFMZy",
        "outputId": "ede3f23a-8882-4ce2-90ac-3eb7e86f02ec"
      },
      "source": [
        "# Test accuracy\n",
        "gold = [0,0,0,0,0,1,1,1,1,1]\n",
        "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
        "accuracy_metric = Accuracy()\n",
        "accuracy = accuracy_metric.binary_accuracy(gold, predicted)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "80.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKVoaWw_vlzo"
      },
      "source": [
        "# FileReader implemented below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JrNQ3J3vh9f"
      },
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "import json\n",
        "\n",
        "class FileReader:\n",
        "  \"\"\"\n",
        "  A class which reads data into pandas dataframe with specified column names.\n",
        "  \"\"\"\n",
        "\n",
        "  @classmethod\n",
        "  def read_tsv_into_pandas(cls, file_path, column_names, header=None, index_col=False, delimiter=\"\\t\"):\n",
        "    \"\"\"\n",
        "    Reads csv into pandas dataframe.\n",
        "\n",
        "    Args:\n",
        "        file_path: path for the train.tsv\n",
        "        column_names: column names to be initiated by pandas dataframe\n",
        "        header: header if wanted - default: None\n",
        "        index_col: whether to index the columns - default: False\n",
        "        delimiter: column seperator - default: \\t\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_csv(file_path, header=header, index_col=index_col, delimiter=delimiter, names=column_names)  \n",
        "\n",
        "    # Don't think we need to drop them\n",
        "    #df = df.drop('story_id', axis='columns', inplace=False)  \n",
        "\n",
        "    return df\n",
        "  \n",
        "  @classmethod\n",
        "  def read_tsv(cls, file_path, quotechar=None, delimiter=\"\\t\"):\n",
        "    \"\"\"\n",
        "    Reads csv into a python list.\n",
        "\n",
        "    Args:\n",
        "        file_path: path for the train.tsv\n",
        "        quotechar: character to quote fields containing special characters - default: None\n",
        "        delimiter: column seperator - default: \\t\n",
        "\n",
        "    Returns:\n",
        "        python list\n",
        "    \"\"\"\n",
        "\n",
        "    df = []\n",
        "    with open(file_path, 'r') as f:\n",
        "      file_reader = csv.reader(f, delimiter=delimiter, quotechar=quotechar)\n",
        "      for l in file_reader:\n",
        "        df.append(l)   \n",
        "\n",
        "    return df  \n",
        "\n",
        "  @classmethod\n",
        "  def read_jsonl_into_pandas(cls, file_path, lines=True):\n",
        "    \"\"\"\n",
        "    Read jsonl into pandas dataframe.\n",
        "\n",
        "    Args:\n",
        "        file_path: path for train.jsonl\n",
        "        lines: whether json or jsonlines file - default: True\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame\n",
        "    \"\"\"\n",
        "\n",
        "    df = pd.read_json(file_path, lines=lines)\n",
        "\n",
        "    # Don't think we need to drop them\n",
        "    #df = df.drop('story_id', axis='columns', inplace=False)\n",
        "\n",
        "    return df\n",
        "  \n",
        "  @classmethod\n",
        "  def read_jsonl(cls, file_path, quotechar=None):\n",
        "    \"\"\"\n",
        "    Reads jsonl into python list.\n",
        "\n",
        "    Args:\n",
        "        file_path: path for the train.jsonl\n",
        "        quotechar: character to quote fields containing special characters - default: None\n",
        "\n",
        "    Returns:\n",
        "        python list\n",
        "    \"\"\"\n",
        "\n",
        "    df = []\n",
        "    with open(file_path, 'rb') as f:\n",
        "      for l in f:\n",
        "        json_obj = json.loads(l)\n",
        "        df.append(json_obj)\n",
        "\n",
        "    return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1prlAXC-SmTP"
      },
      "source": [
        "# PrepareDate class to linguistically preprocess the input data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Tp39A2mFVsk",
        "outputId": "890342d5-c70b-455b-bd97-9f2511ea5790"
      },
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords') \n",
        "\n",
        "class PrepareData:\n",
        "  \"\"\"\n",
        "  A class to preprocess the data.\n",
        "  \"\"\"\n",
        "  def __init__(self, columns=None):\n",
        "    # initialize stopwords, lemmatizer and stemmer\n",
        "    self.stopwords = set(stopwords.words('english'))\n",
        "    self.lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "    self.stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    # set the columns if the input is pandas dataframe\n",
        "    if columns is not None:\n",
        "      self.columns = columns[1:]\n",
        "\n",
        "  def preprocess_text(self, text, flg_clean=False, flg_stemm=False, flg_lemm=False, stopwords=None):\n",
        "    \"\"\"\n",
        "    Procosesses a text with given conditions.\n",
        "\n",
        "    Args:\n",
        "        text: text to be preprocessed\n",
        "        flg_clean: whether to remove non-word characters\n",
        "        flg_stemm: whether to stemm\n",
        "        flg_lemm: whether to lemmatize\n",
        "        stopwords: stopwords to be removed\n",
        "    \n",
        "    Returns:\n",
        "        list of preprocessed tokens\n",
        "    \"\"\"\n",
        "    # remove anything that is not word or space, \n",
        "    if flg_clean:\n",
        "      text = re.sub(r'[^\\w\\s]', '', str(text))\n",
        "            \n",
        "    # strip and lowercase\n",
        "    # whitespace-tokenize - split on space\n",
        "    tokenized = str(text).strip().lower().split()\n",
        "\n",
        "    # remove Stopwords\n",
        "    if stopwords is not None:\n",
        "        tokenized = [word for word in tokenized if word not in \n",
        "                    stopwords]\n",
        "                \n",
        "    # Stemming (remove -ing, -ly, ...)\n",
        "    if flg_stemm == True:\n",
        "        ps = nltk.stem.porter.PorterStemmer()\n",
        "        tokenized = [ps.stem(word) for word in tokenized]\n",
        "                \n",
        "    # Lemmatisation (convert the word into root form)\n",
        "    if flg_lemm == True:\n",
        "        lem = nltk.stem.wordnet.WordNetLemmatizer()\n",
        "        tokenized = [lem.lemmatize(word) for word in tokenized]\n",
        "            \n",
        "    # convert list back to string\n",
        "    #text = \" \".join(tokenized)\n",
        "    return tokenized\n",
        "\n",
        "  def preprocess_data(self, lst_data, flg_clean, flg_stemm, flg_lemm):\n",
        "    \"\"\"\n",
        "    Preprocesses a list containing text.\n",
        "\n",
        "    Args:\n",
        "        lst_data: list to be preprocessed\n",
        "        flg_clean: whether to remove non-word characters\n",
        "        flg_stemm: whether to stemm\n",
        "        flg_lemm: whether to lemmatize\n",
        "    \n",
        "    Returns:\n",
        "        list containing lists of preprocessed tokens\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for line in lst_data:\n",
        "      example = self.preprocess_text(line, flg_clean, flg_stemm, \n",
        "                                                           flg_lemm, self.stopwords)\n",
        "      data.append(example)\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDm4MB1gFmMs"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "class Data:\n",
        "  \"\"\"\n",
        "  A class that represents nli data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, story_id, obs1, hypes, obs2, label=None):\n",
        "    self.story_id = story_id\n",
        "    self.obs1 = obs1\n",
        "    self.hypes = hypes\n",
        "    self.obs2 = obs2\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"Creates printable representation of an nli example.\"\"\"\n",
        "    exp = []\n",
        "    exp.append(\"story_id:\\t{}\".format(self.story_id))\n",
        "    exp.append(\"obs1:\\t{}\".format(self.obs1))\n",
        "    for i, hyp in enumerate(self.hypes, 1):\n",
        "      exp.append(\"hyp{}:\\t{}\".format(i, hyp))\n",
        "    exp.append(\"obs2:\\t{}\".format(self.obs2))\n",
        "\n",
        "    if self.label != None:\n",
        "      exp.append(\"label:\\t{}\".format(self.label))\n",
        "    \n",
        "    return \"\\n \".join(exp)\n",
        "  \n",
        "  def hypothesis_only(self):\n",
        "    \"\"\"Representes the hypothesis only version.\"\"\"\n",
        "    exp = {\"hyp1\": self.hypes[0],\n",
        "           \"hyp2\": self.hypes[1],\n",
        "           \"label\": self.label\n",
        "    }\n",
        "\n",
        "    return exp\n",
        "    \n",
        "  # not really implemented\n",
        "  def obs1_hyp(self):\n",
        "    \"\"\"Represents the first obsevation + hypothesis version.\"\"\"\n",
        "    exp = [\n",
        "           {\"part1\": \" \".join(self.obs1, self.hypes[0])},\n",
        "           {\"part2\": \" \".join(self.obs1, self.hypes[1])}\n",
        "    ]\n",
        "\n",
        "    return exp\n",
        "  \n",
        "  def obs1_hyp_obs2(self):\n",
        "    \"\"\"Represents the first obsevation + hypothesis + second observation version.\"\"\"\n",
        "    exp = {\"hyp1\": self.obs1 + \" \" +  self.hypes[0] + \" \" + self.obs2,\n",
        "            \"hyp2\": self.obs1 + \" \" + self.hypes[1] + \" \" + self.obs2,\n",
        "            \"label\": self.label}\n",
        "\n",
        "    return exp\n",
        "  \n",
        "\n",
        "class DataProcessor(FileReader):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def create_example(self, exp_list, input_type, labels=None,):\n",
        "    \"\"\"Creates examples for the splits.\"\"\"\n",
        "    \n",
        "    examples = []\n",
        "\n",
        "    if labels == None:\n",
        "      labels = [None] * len(exp_list)\n",
        "    \n",
        "    for (i, (exp, label)) in enumerate(zip(exp_list, labels)):\n",
        "      story_id = \"%s\" % (exp['story_id'])\n",
        "\n",
        "      obs1 = exp['obs1']\n",
        "      obs2 = exp['obs2']\n",
        "\n",
        "      hyp1 = exp['hyp1']\n",
        "      hyp2 = exp['hyp2']\n",
        "\n",
        "      label = label\n",
        "\n",
        "      data_raw = Data(story_id=story_id,\n",
        "                  obs1=obs1,\n",
        "                  hypes=[hyp1, hyp2],\n",
        "                  obs2=obs2,\n",
        "                  label=label)\n",
        "      \n",
        "      if input_type is None:\n",
        "        examples.append(data_raw)\n",
        "      \n",
        "      elif input_type is not None:\n",
        "        if input_type == 'hyp-only':\n",
        "          examples.append(data_raw.hypothesis_only())\n",
        "\n",
        "        elif input_type == 'full-seq':\n",
        "          examples.append(data_raw.obs1_hyp_obs2())\n",
        "          \n",
        "    return examples\n",
        "  \n",
        "  def get_labels(self, input_file: str):\n",
        "    labels = []\n",
        "    with open(input_file, \"rb\") as f:\n",
        "        for l in f:\n",
        "            labels.append(l.decode().strip())\n",
        "\n",
        "    return labels\n",
        "  \n",
        "  def create_binary_examples(self, file_path, input_type, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "    \n",
        "    binary_examples = []\n",
        "    binary_labels = []\n",
        "\n",
        "    for example in examples:\n",
        "      if int(example['label']) == 1:\n",
        "        binary_examples.append(example['hyp1'])\n",
        "        binary_labels.append(1)\n",
        "        binary_examples.append(example['hyp2'])\n",
        "        binary_labels.append(0)\n",
        "      elif int(example['label']) == 2:\n",
        "        binary_examples.append(example['hyp2'])\n",
        "        binary_labels.append(1)\n",
        "        binary_examples.append(example['hyp1'])\n",
        "        binary_labels.append(0)\n",
        "\n",
        "    return binary_examples, binary_labels\n",
        "  \n",
        "  def get_train_examples(self, file_path, input_type, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "\n",
        "    return examples\n",
        "  \n",
        "  def get_dev_examples(self, file_path, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_yQm2HwwFFc"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "class Data:\n",
        "  \"\"\"\n",
        "  A class that represents nli data.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, story_id, obs1, hypes, obs2, label=None):\n",
        "    self.story_id = story_id\n",
        "    self.obs1 = obs1\n",
        "    self.hypes = hypes\n",
        "    self.obs2 = obs2\n",
        "    self.label = label\n",
        "\n",
        "  def __repr__(self):\n",
        "    \"\"\"Creates printable representation of an nli example.\"\"\"\n",
        "    exp = []\n",
        "    exp.append(\"story_id:\\t{}\".format(self.story_id))\n",
        "    exp.append(\"obs1:\\t{}\".format(self.obs1))\n",
        "    for i, hyp in enumerate(self.hypes, 1):\n",
        "      exp.append(\"hyp{}:\\t{}\".format(i, hyp))\n",
        "    exp.append(\"obs2:\\t{}\".format(self.obs2))\n",
        "\n",
        "    if self.label != None:\n",
        "      exp.append(\"label:\\t{}\".format(self.label))\n",
        "    \n",
        "    return \"\\n \".join(exp)\n",
        "  \n",
        "  def hypothesis_only(self):\n",
        "    \"\"\"Representes the hypothesis only version.\"\"\"\n",
        "    exp = {\"hyp1\": self.hypes[0],\n",
        "            \"hyp2\": self.hypes[1],\n",
        "            \"label\": self.label\n",
        "           }\n",
        "\n",
        "    return exp\n",
        "  # not really implemented\n",
        "  def obs1_hyp(self):\n",
        "    \"\"\"Represents the first obsevation + hypothesis version.\"\"\"\n",
        "    exp = {\"hyp1\": \" \".join(self.obs1, self.hypes[0]),\n",
        "           \"hyp2\": \" \".join(self.obs1, self.hypes[1])\n",
        "          }\n",
        "\n",
        "    return exp\n",
        "  \n",
        "  def obs1_hyp_obs2(self):\n",
        "    \"\"\"Represents the first obsevation + hypothesis + second observation version.\"\"\"\n",
        "    exp = {\"hyp1\": self.obs1 + \" \" +  self.hypes[0] + \" \" + self.obs2,\n",
        "            \"hyp2\": self.obs1 + \" \" + self.hypes[1] + \" \" + self.obs2,\n",
        "            \"label\": self.label}\n",
        "\n",
        "    return exp\n",
        "  \n",
        "\n",
        "class DataProcessor(FileReader):\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def create_example(self, exp_list, input_type, labels=None,):\n",
        "    \"\"\"Creates examples for the splits.\"\"\"\n",
        "    \n",
        "    examples = []\n",
        "\n",
        "    if labels == None:\n",
        "      labels = [None] * len(exp_list)\n",
        "    \n",
        "    for (i, (exp, label)) in enumerate(zip(exp_list, labels)):\n",
        "      story_id = \"%s\" % (exp['story_id'])\n",
        "\n",
        "      obs1 = exp['obs1']\n",
        "      obs2 = exp['obs2']\n",
        "\n",
        "      hyp1 = exp['hyp1']\n",
        "      hyp2 = exp['hyp2']\n",
        "\n",
        "      label = label\n",
        "\n",
        "      data_raw = Data(story_id=story_id,\n",
        "                  obs1=obs1,\n",
        "                  hypes=[hyp1, hyp2],\n",
        "                  obs2=obs2,\n",
        "                  label=label)\n",
        "      \n",
        "      if input_type is None:\n",
        "        examples.append(data_raw)\n",
        "      \n",
        "      elif input_type is not None:\n",
        "        if input_type == 'hyp-only':\n",
        "          examples.append(data_raw.hypothesis_only())\n",
        "\n",
        "        elif input_type == 'full-seq':\n",
        "          examples.append(data_raw.obs1_hyp_obs2())\n",
        "          \n",
        "    return examples\n",
        "  \n",
        "  def get_labels(self, input_file: str):\n",
        "    labels = []\n",
        "    with open(input_file, \"rb\") as f:\n",
        "        for l in f:\n",
        "            labels.append(l.decode().strip())\n",
        "\n",
        "    return labels\n",
        "  \n",
        "  def create_binary_examples(self, file_path, input_type, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "    \n",
        "    binary_examples = []\n",
        "    binary_labels = []\n",
        "\n",
        "    for example in examples:\n",
        "      if int(example['label']) == 1:\n",
        "        binary_examples.append(example['hyp1'])\n",
        "        binary_labels.append(1)\n",
        "        binary_examples.append(example['hyp2'])\n",
        "        binary_labels.append(0)\n",
        "      elif int(example['label']) == 2:\n",
        "        binary_examples.append(example['hyp2'])\n",
        "        binary_labels.append(1)\n",
        "        binary_examples.append(example['hyp1'])\n",
        "        binary_labels.append(0)\n",
        "    \n",
        "    return binary_examples, binary_labels\n",
        "  \n",
        "  def get_train_examples(self, file_path, input_type, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "\n",
        "    return examples\n",
        "  \n",
        "  def get_dev_examples(self, file_path, labels_path=None):\n",
        "    if labels_path != None:\n",
        "      examples = self.create_example(self.read_jsonl(file_path), input_type, self.get_labels(labels_path))\n",
        "    else:\n",
        "      examples = self.create_example(self.read_jsonl(file_path))\n",
        "\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHh741jtVzPi"
      },
      "source": [
        "# Class to create GloVe word embeddings for training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnSv1Yc-Fpty"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "class Features:\n",
        "  \"\"\"\n",
        "  A class for future extraction.\n",
        "  ...\n",
        "  Param:\n",
        "      embedding_file: GloVe embedding file\n",
        "      lst_corpus: list of training examples\n",
        "      embedding_dim: embedding dimensions\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, embedding_file, lst_corpus, embedding_dim):\n",
        "    self.embedding_file = embedding_file\n",
        "    self.lst_corpus = lst_corpus\n",
        "    self.embedding_dim = embedding_dim\n",
        "  \n",
        "  def get_word_index(self):\n",
        "    \"\"\"\n",
        "    Creates a word index from the corpus.\n",
        "    \"\"\"\n",
        "\n",
        "    vocab = set()\n",
        "    for l in self.lst_corpus:\n",
        "      for t in l:\n",
        "        vocab.add(t)\n",
        "\n",
        "    word_index = dict()\n",
        "    for i,w in enumerate(vocab):\n",
        "      word_index[w] = i\n",
        "\n",
        "    return word_index\n",
        "  \n",
        "  def create_embedding_index(self):\n",
        "    \"\"\"\n",
        "    Creates an embedding index for GloVe embeddings.\n",
        "    \"\"\"\n",
        "\n",
        "    embeddings_index = {}\n",
        "    f = open(os.path.join('./', self.embedding_file))\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "    #print('Found %s word vectors.' % len(embeddings_index))\n",
        "    return embeddings_index\n",
        "\n",
        "  def create_embedding_matrix(self):\n",
        "    \"\"\"\n",
        "    Creates an embedding matrix given the word index and the embedding index.\n",
        "    \"\"\"\n",
        "\n",
        "    word_index = self.get_word_index()\n",
        "    embeddings_index = self.create_embedding_index()\n",
        "    embedding_matrix = np.zeros((len(word_index) + 1, self.embedding_dim))\n",
        "    for word,i in word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        \n",
        "    return embedding_matrix \n",
        "  \n",
        "  def create_features(self):\n",
        "    \"\"\"\n",
        "    Creates the GloVe words embeddings from the training data.\n",
        "    Averages the word embeddings to create an averaged bag of words.\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_matrix = self.create_embedding_matrix()\n",
        "    word_index = self.get_word_index()\n",
        "    sentence_embeddings = []\n",
        "    for s in self.lst_corpus:\n",
        "      summed = np.zeros(self.embedding_dim)\n",
        "      for w in s:\n",
        "        w_embedding = embedding_matrix[word_index.get(w)]\n",
        "        summed += w_embedding\n",
        "      sentence_embeddings.append(summed/len(s))\n",
        "    \n",
        "    return sentence_embeddings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwHdg7wDFyTX"
      },
      "source": [
        "Download GloVe embeddings...\n",
        "\n",
        "Unzip the file..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQwAm6V_F3Od",
        "outputId": "6669eee8-e056-46f8-8d62-d98fabc9a73f"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 16:08:26--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-05-26 16:08:26--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-05-26 16:08:26--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.05MB/s    in 2m 46s  \n",
            "\n",
            "2021-05-26 16:11:13 (4.95 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIsFZuQMYGnQ"
      },
      "source": [
        "Download the data...\n",
        "\n",
        "Unzip the file..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9n5DiC_YFta",
        "outputId": "d3748970-ab48-4c9e-99b7-3aee8adb3973"
      },
      "source": [
        "!wget https://storage.googleapis.com/ai2-mosaic/public/alphanli/alphanli-train-dev.zip\n",
        "!unzip -d alphanli alphanli-train-dev.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-05-26 16:11:37--  https://storage.googleapis.com/ai2-mosaic/public/alphanli/alphanli-train-dev.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.193.128, 172.217.204.128, 172.217.203.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.193.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5118294 (4.9M) [application/zip]\n",
            "Saving to: ‘alphanli-train-dev.zip’\n",
            "\n",
            "alphanli-train-dev. 100%[===================>]   4.88M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-05-26 16:11:38 (67.8 MB/s) - ‘alphanli-train-dev.zip’ saved [5118294/5118294]\n",
            "\n",
            "Archive:  alphanli-train-dev.zip\n",
            "  inflating: alphanli/train.jsonl    \n",
            "  inflating: alphanli/train-labels.lst  \n",
            "  inflating: alphanli/dev.jsonl      \n",
            "  inflating: alphanli/dev-labels.lst  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oy8irLu6XAzD"
      },
      "source": [
        "Set the paths to the data..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sv64-WvYGqfz"
      },
      "source": [
        "train_file = './alphanli/train.jsonl'\n",
        "train_labels = './alphanli/train-labels.lst'\n",
        "dev_file = './alphanli/dev.jsonl'\n",
        "dev_labels = './alphanli/dev-labels.lst'\n",
        "test_file = './alphanli/test.jsonl'\n",
        "test_labels = './alphanli/test-labels.lst'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCUE9wSFXItx"
      },
      "source": [
        "Get the examples and the corresponding lables..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVH8A3dvtkgr"
      },
      "source": [
        "data_processor = DataProcessor()\n",
        "# get the training data for hypothesis-only experiment\n",
        "train_exp, train_labels = data_processor.create_binary_examples(file_path=train_file, input_type='hyp-only', labels_path=train_labels)\n",
        "\n",
        "# get the training data for fully-connected experiment\n",
        "# train_exp, train_labels = data_processor.create_binary_examples(file_path=train_file, input_type='hyp-only', labels_path=train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hesoZ-uaBRA"
      },
      "source": [
        "# get the dev data for hypothesis-only experiment\n",
        "dev_exp, dev_labels = data_processor.create_binary_examples(file_path=dev_file, input_type='hyp-only', labels_path=dev_labels)\n",
        "\n",
        "# get the dev data for fully-connected experiment\n",
        "# dev_exp, dev_labels = data_processor.create_binary_examples(file_path=dev_file, input_type='full-seq', labels_path=dev_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEY0xJDI2tOm"
      },
      "source": [
        "# get the test data for hypothesis-only experiment\n",
        "test_exp, test_labels = data_processor.create_binary_examples(file_path=test_path, input_type='hypo-only', labels_path=test_labels)\n",
        "\n",
        "# get the test data for fully-connected experiment\n",
        "# test_exp, test_labels = data_processor.create_binary_examples(file_path=test_path, input_type='full-seq', labels_path=test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAv0LJ_1XPjZ"
      },
      "source": [
        "Normalize the training examples..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hngRCUNGGEGp"
      },
      "source": [
        "processor = PrepareData()\n",
        "normalized_train = processor.preprocess_data(train_exp, flg_clean=True, flg_stemm=False, flg_lemm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErWH3kPBaf20"
      },
      "source": [
        "normalized_dev = processor.preprocess_data(dev_exp, flg_clean=True, flg_stemm=False, flg_lemm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqv3j6IgAxfg"
      },
      "source": [
        "normalized_test = processor.preprocess_data(test_exp, flg_clean=True, flg_stemm=False, flg_lemm=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PzQCw-BXVk9"
      },
      "source": [
        "Create the averaged glove sentence embeddings...\n",
        "\n",
        "\n",
        "\n",
        "*  Encode words\n",
        "*  Average the words in the sequence to create final embeddings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prEkpskrJhLC"
      },
      "source": [
        "feature_extractor_train = Features(embedding_file='glove.6B.100d.txt', lst_corpus=normalized_train, embedding_dim=100)\n",
        "feature_extractor_dev = Features(embedding_file='glove.6B.100d.txt', lst_corpus=normalized_dev, embedding_dim=100)\n",
        "feature_extractor_test = Features(embedding_file='glove.6B.100d.txt', lst_corpus=normalized_test, embedding_dim=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vKBazrHJhNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5d0869-3cb9-4081-8d62-5439c0374b04"
      },
      "source": [
        "sentence_vectors_train = feature_extractor_train.create_features()\n",
        "sentence_vectors_dev = feature_extractor_dev.create_features()\n",
        "sentence_vectors_test = feature_extractor_test.create_features()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: RuntimeWarning: invalid value encountered in true_divide\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtBdiefvJp_S"
      },
      "source": [
        "# Training Data:\n",
        "* X_train\n",
        "* y_train\n",
        "\n",
        "# Development Data:\n",
        "* X_dev\n",
        "* y_dev\n",
        "\n",
        "# Test Data:\n",
        "* X_test\n",
        "* y_test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuQu7yRT7Wkr"
      },
      "source": [
        "X_train = sentence_vectors_train\n",
        "y_train = labels_train\n",
        "\n",
        "X_dev = sentence_vectors_dev\n",
        "y_dev = labels_dev\n",
        "\n",
        "X_test = sentence_vectors_test\n",
        "y_test = labels_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3n4mZdZBxtIX"
      },
      "source": [
        "The training data is ordered.\n",
        "\n",
        "Shuffle sentence_vectors and labels before training.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3HkMSZ9yESp"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "X_train, y_train = shuffle(X_train, y_train)\n",
        "X_dev, y_dev = shuffle(X_dev, y_dev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHDzNypNbdil"
      },
      "source": [
        "# Till here it's almost good to go. I only need to do one last thing tomorrow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmOUF5C56V-U",
        "outputId": "197612f2-a40d-4bf5-e3b8-26238ea5f187"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class BaseLayer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input\n",
        "\n",
        "    def backward(self, input, gradients):\n",
        "        # d(loss) / d(x) = gradients * ( d(layer) / d(x) )\n",
        "        d_layer_d_x = np.eye(input.shape[1])\n",
        "        d_loss_d_x = np.dot(gradients, d_layer_d_x)\n",
        "        return d_loss_d_x\n",
        "\n",
        "    def update(self, weights_grads, bias_grads):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Dense(BaseLayer):\n",
        "    def __init__(self, shape, lr=0.01):\n",
        "        # Xavier initialization on ReLu using nets\n",
        "        self.weights = np.random.normal(loc=0, scale=1, size=shape)*np.sqrt(2/shape[1])\n",
        "        self.bias = np.zeros(shape[1])\n",
        "        self.lr = lr\n",
        "\n",
        "    def forward(self, input):\n",
        "        return np.dot(input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, input, gradients):\n",
        "        # d f / d x = (d f / d layer) * (d layer / d x)\n",
        "        d_dense_d_input = np.dot(gradients, self.weights.T)\n",
        "        weights_grads = np.dot(input.T, gradients)\n",
        "        bias_grads = gradients.mean(axis=0)*input.shape[0]\n",
        "        self.update(weights_grads, bias_grads)\n",
        "        return d_dense_d_input\n",
        "\n",
        "    def update(self, weights_grads, bias_grads):\n",
        "        self.weights = self.weights - self.lr * weights_grads\n",
        "        self.bias = self.bias - self.lr * bias_grads\n",
        "\n",
        "\n",
        "class ReLU(BaseLayer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        logits = np.maximum(0, input)\n",
        "        return logits\n",
        "\n",
        "    def backward(self, input, gradients):\n",
        "        return input * (input > 0)\n",
        "\n",
        "\n",
        "class MultiLayerPerceptron:\n",
        "    def __init__(self):\n",
        "        self.layers = list()\n",
        "\n",
        "    def add_layer(self, shape):\n",
        "        self.layers.append(Dense(shape))\n",
        "\n",
        "    def add_activation_func(self):\n",
        "        self.layers.append(ReLU())\n",
        "\n",
        "    def forward(self, input):\n",
        "        intermediate_outputs = list()\n",
        "        this_input = input\n",
        "        if len(self.layers) > 0:\n",
        "            for layer in self.layers:\n",
        "                intermediate_outputs.append(layer.forward(this_input))\n",
        "                this_input = intermediate_outputs[-1]\n",
        "        else:\n",
        "            raise ValueError(\"Must have at least one layer!\")\n",
        "        return intermediate_outputs\n",
        "\n",
        "    def loss_n_grads(self, init_grads, y):\n",
        "        # loss = -[golden] + log (SUM([predict n]))\n",
        "        x_id = np.arange(len(init_grads))\n",
        "        golden_grads = init_grads[x_id, y]\n",
        "\n",
        "        golden_labels = np.zeros(init_grads.shape, dtype=init_grads.dtype)\n",
        "        golden_labels[x_id, y] = 1\n",
        "        loss = - golden_grads + np.log(np.sum(np.exp(init_grads), axis=-1))\n",
        "\n",
        "        # keepdims = True very important! Takes forever to debug this.\n",
        "        softmax = np.exp(init_grads) / (np.exp(init_grads).sum(keepdims=True, axis=-1))\n",
        "        loss_grads = (softmax-golden_labels) / init_grads.shape[0]\n",
        "\n",
        "        return loss, loss_grads\n",
        "\n",
        "    def train(self, x, y):\n",
        "        intermediate_outputs = self.forward(x)\n",
        "        initial_grads = intermediate_outputs[-1]\n",
        "        loss, loss_grads = self.loss_n_grads(initial_grads, y)\n",
        "\n",
        "        intermediate_inputs = [x] + intermediate_outputs\n",
        "        for layer_id in reversed(range(len(self.layers))):\n",
        "            loss_grads = self.layers[layer_id].backward(intermediate_inputs[layer_id], loss_grads)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.forward(x)[-1].argmax(axis=-1)\n",
        "\n",
        "    def validate(self, x, y):\n",
        "        return np.mean(self.forward(x)[-1].argmax(axis=-1) == y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.51953125\n",
            "0.517578125\n",
            "0.517578125\n",
            "0.517578125\n",
            "0.517578125\n",
            "0.515625\n",
            "0.515625\n",
            "0.515625\n",
            "0.515625\n",
            "0.515625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR6jMPpYqI9a"
      },
      "source": [
        "# Training loop\n",
        "##### we do not have dev set, so here i'll just train the model without validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxsCj_V26Cg2",
        "outputId": "61cafdd2-2200-4099-f95b-e1b9e1457341"
      },
      "source": [
        "from statistics import mean\n",
        "\n",
        "# A functiion which returns a generator, yielding one batch of data each time\n",
        "def make_batch(data, batch_size):\n",
        "  batch_data = []\n",
        "  for entry in data:\n",
        "    if entry is not None:\n",
        "      batch_data.append(entry)\n",
        "      if len(batch_data) == batch_size:\n",
        "        yield np.asarray(batch_data)\n",
        "        batch_data = []\n",
        "    else:\n",
        "      continue\n",
        "  if batch_data:\n",
        "    yield np.asarray(batch_data)\n",
        "\n",
        "parameter_size = 150\n",
        "max_epoch = 20\n",
        "batch_size = 1024\n",
        "\n",
        "# If the accuracy on dev set does not increase in the latest 3 epoches of training, stop training early\n",
        "early_stopping = 3 \n",
        "\n",
        "\n",
        "# Building the model: 2 dense layers and one ReLu layer as activation function\n",
        "model = MultiLayerPerceptron()\n",
        "model.add_layer((100, parameter_size))\n",
        "model.add_activation_func()\n",
        "model.add_layer((parameter_size, 2))\n",
        "\n",
        "# The training loop\n",
        "dev_acces = []\n",
        "for e in range(1, max_epoch):\n",
        "  for X_train_batch, y_train_batch in zip(make_batch(X_train, batch_size), make_batch(y_train, batch_size)):\n",
        "    model.train(X_train_batch, y_train_batch)\n",
        "\n",
        "  epoch_dev_acc = []\n",
        "  for X_dev_batch, y_dev_batch in zip(make_batch(X_dev, batch_size), make_batch(y_dev, batch_size)):\n",
        "    this_acc = model.validate(X_dev_batch, y_dev_batch)\n",
        "    epoch_dev_acc.append(this_acc)\n",
        "  this_dev_acc = mean(epoch_dev_acc)\n",
        "\n",
        "  dev_acces.append(this_dev_acc)\n",
        "  print(\"dev acc: \", this_dev_acc)\n",
        "  if len(dev_acces) == early_stopping:\n",
        "    if dev_acces[2] <= dev_acces[1] and dev_acces[2] <= dev_acces[0]:\n",
        "      break\n",
        "    else:\n",
        "      dev_acces = []\n",
        "      continue\n",
        "    \n",
        "predict = model.predict(X_test)\n",
        "acc = np.mean(predict == y_test)\n",
        "\n",
        "print(\"acc: \", this_acc, \"acc on dev:\", this_dev_acc, \"e: \", e)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n",
            "0.5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}